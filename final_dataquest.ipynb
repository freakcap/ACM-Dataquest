{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Target days for resolving of a query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING REQUIRED LIBRARIES, MODULES AND DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "sns.set()\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA PREPROCESSING AND FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "train['opened_at'] = pd.to_datetime(train.opened_at)\n",
    "train['month'] = train['opened_at'].dt.month\n",
    "train['dayofweek'] = train['opened_at'].dt.dayofweek\n",
    "\n",
    "# train['year'] = train['opened_at'].dt.year\n",
    "# train['day'] = train['opened_at'].dt.day\n",
    "# train['dayofyear'] = train['opened_at'].dt.dayofyear\n",
    "# train['weekofyear'] = train['opened_at'].dt.weekofyear\n",
    "# train['hour'] = train['opened_at'].dt.hour\n",
    "# train['minute'] = train['opened_at'].dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers and unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in train.iterrows():\n",
    "    if(row['update_count']!=0):\n",
    "        train.drop(index,inplace=True)  # LESS THAN 10 VALUES\n",
    "    elif(row['vendor']=='Vendor 1' or row['vendor']=='code 8s'):\n",
    "        train.drop(index,inplace=True)  # ONLY 5 VALUES \n",
    "    elif(row['knowledge']=='True'):\n",
    "        train.drop(index,inplace=True)  # LESS THAN 20 IN TRAIN AND NONE IN TEST\n",
    "    elif(row['made_sla']=='False'):\n",
    "        train.drop(index,inplace=True)  # ONLY ONE IN TRAIN AND NONE IN TEST\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in train.iterrows(): \n",
    "    if(row['target_days']>10):\n",
    "        train.drop(index,inplace=True)\n",
    "for index,row in train.iterrows(): \n",
    "    if(row['target_days']<=4):\n",
    "        train.drop(index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNING CUSTOM WEIGHTS FROM 0 TO 11 BY VISUALIZING BAR PLOT\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    if train.loc[index,'month']==3:\n",
    "        train.at[index,'month']=11\n",
    "    elif train.loc[index,'month']==4:\n",
    "        train.at[index,'month']=5\n",
    "    elif train.loc[index,'month']==5:\n",
    "        train.at[index,'month']=2\n",
    "    elif train.loc[index,'month']==2:\n",
    "        train.at[index,'month']=1\n",
    "    else:\n",
    "        train.at[index,'month']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING OF UNNECESSARY COLUMNS\n",
    "\n",
    "\n",
    "train.drop(['vendor'], axis=1, inplace=True)\n",
    "train.drop(['reassignment_count'], axis=1, inplace=True)   # ALL ZEROES\n",
    "train.drop(['reopen_count'], axis=1, inplace=True)         # ALL ZEROES\n",
    "train.drop(['update_count'], axis=1, inplace=True)         # ALL ZEROES\n",
    "# train.drop(['assigned_to'], axis=1, inplace=True)\n",
    "train.drop(['opened_at'], axis=1, inplace=True)            # TIMESTAMP\n",
    "train.drop(['Id'], axis=1, inplace=True)                   # ALL UNIQUE VALUES\n",
    "train.drop(['made_sla'], axis=1, inplace=True)\n",
    "train.drop(['notify'], axis=1, inplace=True)               # NO SIGNIFICANCE IN TEST\n",
    "train.drop(['contact_type'], axis=1, inplace=True)         # FEATURES IN TEST FAR DIFFERENT THAN TRAIN\n",
    "train.drop(['impact'], axis=1, inplace=True)               # CONTRIBUTE TO PRIORITY\n",
    "train.drop(['urgency'], axis=1, inplace=True)\n",
    "# train.drop(['priority'], axis=1, inplace=True)\n",
    "# train.drop(['location'], axis=1, inplace=True)\n",
    "train.drop(['knowledge'], axis=1, inplace=True)\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting of columns to seperate numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['OpBy','opby','opened_by1']] = train.opened_by.str.split(expand=True) \n",
    "train[['loc','location1']] = train.location.str.split(expand=True) \n",
    "train[['cat','category1']] = train.category.str.split(expand=True) \n",
    "train[['scat','subcategory1']] = train.subcategory.str.split(expand=True) \n",
    "train[['assg','assigned_to1']] = train.assigned_to.str.split(expand=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING UNNECESSARILY CREATED COLUMNS\n",
    "\n",
    "\n",
    "train.drop(['OpBy'], axis=1, inplace=True)\n",
    "train.drop(['opby'], axis=1, inplace=True)\n",
    "train.drop(['opened_by'], axis=1, inplace=True)\n",
    "train['opened_by1'] = train['opened_by1'].astype(float)\n",
    "\n",
    "train.drop(['loc'], axis=1, inplace=True)\n",
    "train.drop(['location'], axis=1, inplace=True)\n",
    "train['location1'] = train['location1'].astype(float)\n",
    "\n",
    "train.drop(['cat'], axis=1, inplace=True)\n",
    "train.drop(['category'], axis=1, inplace=True)\n",
    "train['category1'] = train['category1'].astype(float)\n",
    "\n",
    "train.drop(['scat'], axis=1, inplace=True)\n",
    "train.drop(['subcategory'], axis=1, inplace=True)\n",
    "train['subcategory1'] = train['subcategory1'].astype(float)\n",
    "\n",
    "train.drop(['assg'], axis=1, inplace=True)\n",
    "train.drop(['assigned_to'], axis=1, inplace=True)\n",
    "train['assigned_to1'] = train['assigned_to1'].astype(float)\n",
    "# train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[['impact1','imp']] = train['impact'].str.split('-',expand=True)\n",
    "# train.drop(['imp'], axis=1, inplace=True)\n",
    "# train.drop(['impact'], axis=1, inplace=True)\n",
    "# train['impact1'] = train['impact1'].astype(float)\n",
    "\n",
    "# train[['urgency1','urg']] = train['urgency'].str.split('-',expand=True)\n",
    "# train.drop(['urg'], axis=1, inplace=True)\n",
    "# train.drop(['urgency'], axis=1, inplace=True)\n",
    "# train['urgency1'] = train['urgency1'].astype(float)\n",
    "\n",
    "train[['priority1','pri']] = train['priority'].str.split('-',expand=True)\n",
    "train.drop(['pri'], axis=1, inplace=True)\n",
    "train.drop(['priority'], axis=1, inplace=True)\n",
    "train['priority1'] = train['priority1'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUES FILLED BY CONSIDERING MOST FREQUENT VALUE IN THE SPECIFIC CATEGORY BY GROUPING USING TARGET_DAYS\n",
    "\n",
    "\n",
    "train['category1'].fillna(train.groupby('target_days')['category1'].transform('value_counts'), inplace=True)\n",
    "train['assigned_to1'].fillna(train.groupby('target_days')['assigned_to1'].transform('value_counts'), inplace=True)\n",
    "train['subcategory1'].fillna(train.groupby('target_days')['subcategory1'].transform('value_counts'), inplace=True)\n",
    "train['location1'].fillna(train.groupby('target_days')['location1'].transform('value_counts'), inplace=True)\n",
    "train['opened_by1'].fillna(train.groupby('target_days')['opened_by1'].transform('value_counts'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(axis = 0, how ='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['RAC'] = train['opened_by1'].map(str)+\"\"+train['location1'].map(str)+\"\"+train['category1'].map(str)+\"\"+train['subcategory1'].map(str)+\"\"+train['priority1'].map(str)\n",
    "# train['ROC'] = train['location1'].map(str)+\"\"+train['category1'].map(str)+\"\"+train['subcategory1'].map(str)+\"\"+train['priority1'].map(str)+\"\"+train['assigned_to1'].map(str)\n",
    "# train['UC'] = train['opened_by1'].map(str)+\"\"+train['location1'].map(str)+\"\"+train['category1'].map(str)+\"\"+train['subcategory1'].map(str)+\"\"+train['assigned_to1'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = train['RAC'].value_counts() \n",
    "# z1 = z.to_dict() #converts to dictionary\n",
    "\n",
    "# train['reassignment_count'] = train['RAC'].map(z1) \n",
    "\n",
    "# x = train['ROC'].value_counts() \n",
    "# x1 = x.to_dict() #converts to dictionary\n",
    "\n",
    "# train['reopen_count'] = train['ROC'].map(x1) \n",
    "\n",
    "# y = train['UC'].value_counts() \n",
    "# y1 = y.to_dict() #converts to dictionary\n",
    "\n",
    "# train['update_count'] = train['UC'].map(y1) \n",
    "\n",
    "# train.drop(['RAC'], axis=1, inplace=True)\n",
    "# train.drop(['ROC'], axis=1, inplace=True)\n",
    "# train.drop(['UC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"prepros.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = train[\"target_days\"]\n",
    "\n",
    "X = train.drop([\"target_days\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SIZE WAS 0.2 WHILE TESTING, WHILE FINAL SUBMISSION WAS MADE ON MINIMUM TEST SIZE\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train , x_test, y_train, y_test = train_test_split(X,y,test_size=0.0005,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler=StandardScaler()\n",
    "# X_train = scaler.fit_transform(x_train)\n",
    "# X_test = scaler.transform(x_test)\n",
    "X_train = x_train\n",
    "X_test = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HYPERTUNING OF PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter hypertuning using random grid and k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 100)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [2, 4,8]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]# Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestRegressor()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "# rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. APPLYING PROPER ML MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor with tuned parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=100,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=4, min_samples_split=5,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=944,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "lr = RandomForestRegressor(bootstrap=True,n_estimators=944,max_features='auto',min_samples_leaf=4,min_samples_split=5,max_depth=100)\n",
    "\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.4700664472944923\n",
      "MAE: 0.4550714182718867\n",
      "MSE: 1.0382371644842603\n",
      "LMSE: 0.014132753863816211\n",
      "RMSE: 1.0189392349322213\n",
      "LRMSE: 0.11888125951476208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error,r2_score\n",
    "y_pred= lr.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "lmse = mean_squared_log_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "lrmse = np.sqrt(lmse)\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"LMSE:\", lmse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"LRMSE:\", lrmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DATA PREPROCESSING FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESTAMP FEATURE EXTRACTION\n",
    "\n",
    "from datetime import datetime\n",
    "test['opened_at'] = pd.to_datetime(test.opened_at)\n",
    "test['month'] = test['opened_at'].dt.month\n",
    "test['dayofweek'] = test['opened_at'].dt.dayofweek\n",
    "\n",
    "# test['year'] = test['opened_at'].dt.year\n",
    "# test['day'] = test['opened_at'].dt.day\n",
    "# test['dayofyear'] = test['opened_at'].dt.dayofyear\n",
    "# test['weekofyear'] = test['opened_at'].dt.weekofyear\n",
    "# test['hour'] = test['opened_at'].dt.hour\n",
    "# test['minute'] = test['opened_at'].dt.minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM WEIGHTS SAME AS TRAIN\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    if test.loc[index,'month']==3:\n",
    "        test.at[index,'month']=11\n",
    "    elif test.loc[index,'month']==4:\n",
    "        test.at[index,'month']=5\n",
    "    elif test.loc[index,'month']==5:\n",
    "        test.at[index,'month']=2\n",
    "    elif test.loc[index,'month']==2:\n",
    "        test.at[index,'month']=1\n",
    "    else:\n",
    "        test.at[index,'month']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING UNNECESSARY COLUMNS SAME AS TRAIN\n",
    "\n",
    "Id = test['Id']\n",
    "\n",
    "test.drop(['vendor'], axis=1, inplace=True)\n",
    "test.drop(['reassignment_count'], axis=1, inplace=True)\n",
    "test.drop(['reopen_count'], axis=1, inplace=True)\n",
    "test.drop(['update_count'], axis=1, inplace=True)\n",
    "# test.drop(['assigned_to'], axis=1, inplace=True)\n",
    "test.drop(['opened_at'], axis=1, inplace=True)\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "test.drop(['made_sla'], axis=1, inplace=True)\n",
    "test.drop(['notify'], axis=1, inplace=True)\n",
    "test.drop(['contact_type'], axis=1, inplace=True)\n",
    "test.drop(['knowledge'], axis=1, inplace=True)\n",
    "# test.drop(['location'], axis=1, inplace=True)\n",
    "test.drop(['impact'], axis=1, inplace=True)\n",
    "test.drop(['urgency'], axis=1, inplace=True)\n",
    "\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING OF COLUMNS FOR SEPERATING NUMERIC VALUES\n",
    "\n",
    "test[['OpBy','opby','opened_by1']] = test.opened_by.str.split(expand=True) \n",
    "test[['loc','location1']] = test.location.str.split(expand=True) \n",
    "test[['cat','category1']] = test.category.str.split(expand=True) \n",
    "test[['scat','subcategory1']] = test.subcategory.str.split(expand=True) \n",
    "test[['assg','assigned_to1']] = test.assigned_to.str.split(expand=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING UNNECESSARY COLUMNS\n",
    "\n",
    "test.drop(['OpBy'], axis=1, inplace=True)\n",
    "test.drop(['opby'], axis=1, inplace=True)\n",
    "test.drop(['opened_by'], axis=1, inplace=True)\n",
    "test['opened_by1'] = test['opened_by1'].astype(float)\n",
    "\n",
    "test.drop(['loc'], axis=1, inplace=True)\n",
    "test.drop(['location'], axis=1, inplace=True)\n",
    "test['location1'] = test['location1'].astype(float)\n",
    "\n",
    "test.drop(['cat'], axis=1, inplace=True)\n",
    "test.drop(['category'], axis=1, inplace=True)\n",
    "test['category1'] = test['category1'].astype(float)\n",
    "\n",
    "test.drop(['scat'], axis=1, inplace=True)\n",
    "test.drop(['subcategory'], axis=1, inplace=True)\n",
    "test['subcategory1'] = test['subcategory1'].astype(float)\n",
    "\n",
    "test.drop(['assg'], axis=1, inplace=True)\n",
    "test.drop(['assigned_to'], axis=1, inplace=True)\n",
    "test['assigned_to1'] = test['assigned_to1'].astype(float)\n",
    "# test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[['impact1','imp']] = test['impact'].str.split('-',expand=True)\n",
    "# test.drop(['imp'], axis=1, inplace=True)\n",
    "# test.drop(['impact'], axis=1, inplace=True)\n",
    "# test['impact1'] = test['impact1'].astype(float)\n",
    "\n",
    "# test[['urgency1','urg']] = test['urgency'].str.split('-',expand=True)\n",
    "# test.drop(['urg'], axis=1, inplace=True)\n",
    "# test.drop(['urgency'], axis=1, inplace=True)\n",
    "# test['urgency1'] = test['urgency1'].astype(float)\n",
    "\n",
    "test[['priority1','pri']] = test['priority'].str.split('-',expand=True)\n",
    "test.drop(['pri'], axis=1, inplace=True)\n",
    "test.drop(['priority'], axis=1, inplace=True)\n",
    "test['priority1'] = test['priority1'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILLING MISSING VALUES BY THE MOST FREQUENT VALUE IN THAT CATEGORY BY GROUPING USING LOCATION\n",
    "\n",
    "test['category1'].fillna(test.groupby('location1')['category1'].transform('value_counts'), inplace=True)\n",
    "test['assigned_to1'].fillna(test.groupby('location1')['assigned_to1'].transform('value_counts'), inplace=True)\n",
    "test['subcategory1'].fillna(test.groupby('location1')['subcategory1'].transform('value_counts'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['RAC'] = test['opened_by1'].map(str)+\"\"+test['location1'].map(str)+\"\"+test['category1'].map(str)+\"\"+test['subcategory1'].map(str)+\"\"+test['priority1'].map(str)\n",
    "# test['ROC'] = test['location1'].map(str)+\"\"+test['category1'].map(str)+\"\"+test['subcategory1'].map(str)+\"\"+test['priority1'].map(str)+\"\"+test['assigned_to1'].map(str)\n",
    "# test['UC'] = test['opened_by1'].map(str)+\"\"+test['location1'].map(str)+\"\"+test['category1'].map(str)+\"\"+test['subcategory1'].map(str)+\"\"+test['assigned_to1'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = test['RAC'].value_counts() \n",
    "# z1 = z.to_dict() #converts to dictionary\n",
    "\n",
    "# test['reassignment_count'] = test['RAC'].map(z1) \n",
    "\n",
    "# x = test['ROC'].value_counts() \n",
    "# x1 = x.to_dict() #converts to dictionary\n",
    "\n",
    "# test['reopen_count'] = test['ROC'].map(x1) \n",
    "\n",
    "# y = test['UC'].value_counts() \n",
    "# y1 = y.to_dict() #converts to dictionary\n",
    "\n",
    "# test['update_count'] = test['UC'].map(y1) \n",
    "\n",
    "# test.drop(['RAC'], axis=1, inplace=True)\n",
    "# test.drop(['ROC'], axis=1, inplace=True)\n",
    "# test.drop(['UC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler=StandardScaler()\n",
    "# test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PREDICTING FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test=lr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test=prediction_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4918, 1)\n"
     ]
    }
   ],
   "source": [
    "my_solution=pd.DataFrame(prediction_test,Id,columns=[\"target_days\"])\n",
    "print(my_solution.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SAVING THE PREDICTIONS IN CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_solution.to_csv(\"prediction21.csv\",index_label=[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4918, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_solution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3570\n",
       "6    1163\n",
       "7     167\n",
       "8      17\n",
       "9       1\n",
       "Name: target_days, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_solution['target_days'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
